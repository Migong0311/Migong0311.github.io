


# 🧠 **Day 05 — 토크나이저·임베딩·RNN·LSTM·Attention·HuggingFace (시험 대비 정리)**

---

## 📘 1️⃣ **Tokenizer (토크나이저)**

### 🔹 핵심 개념

자연어 문장을 **단어(토큰)** 단위로 분리하고, 이를 **숫자 시퀀스(ID)** 로 변환하는 과정입니다.
딥러닝 모델은 텍스트를 직접 이해할 수 없기 때문에 → **숫자 기반 벡터 표현**으로 바꿔야 합니다.

| 단계        | 설명                    | 대표 함수              |
| ----------- | ----------------------- | ---------------------- |
| 토큰화      | 문장을 단어 단위로 분리 | `tokenizer.tokenize()` |
| 정수 인코딩 | 단어 → 정수 ID 변환     | `tokenizer.encode()`   |
| 디코딩      | 정수 ID → 단어 복원     | `tokenizer.decode()`   |

> 예: `"I love AI"` → `[101, 1045, 2293, 9936, 102]`

---

### 🔹 주요 토크나이징 방식 비교

| 방식                         | 특징                       | 장점                 | 단점                     |
| ---------------------------- | -------------------------- | -------------------- | ------------------------ |
| **Word-level**               | 단어 단위                  | 빠름                 | 신조어 처리 불가         |
| **Character-level**          | 문자 단위                  | 희귀어 처리 가능     | 시퀀스 길이 김           |
| **BPE (Byte Pair Encoding)** | 자주 등장하는 문자 쌍 병합 | 신조어 처리 가능     | 희귀 단어 분리 과다 가능 |
| **WordPiece**                | BERT 계열 모델 사용        | 안정적인 희귀어 처리 | 학습 데이터에 의존적     |

> 💡 **BPE vs WordPiece**
>
> * BPE: 빈도 기반 병합
> * WordPiece: 확률 기반(언어 모델 가능도 기반)

---

### 🔹 주요 면접 포인트

* “토크나이저는 어떤 기준으로 문장을 분리하나요?”
  → 공백, 구두점, 서브워드 빈도 기반
* “BPE와 WordPiece의 차이는?”
  → BPE는 빈도 기반, WordPiece는 확률 기반

---

## 📘 2️⃣ **토큰 → 숫자 ID 변환**

### 🔹 핵심 포인트

`encode()`는 문장을 정수 ID 시퀀스로 변환,
`decode()`는 숫자 시퀀스를 다시 문장으로 복원.

| 토큰 예시 | ID 시퀀스                    | 특수 토큰 용도      |
| --------- | ---------------------------- | ------------------- |
| `[CLS]`   | 문장 시작 (classification용) | 문장 전체 요약 입력 |
| `[SEP]`   | 문장 구분                    | 두 문장 입력 구분   |
| `[PAD]`   | 패딩용                       | 길이 맞추기용       |

> ✅ 시험 포인트:
> “특수 토큰은 모델 입력 정규화를 위해 사용된다.”

---

## 📘 3️⃣ **Word Embedding (임베딩)**

### 🔹 개념

토큰 ID를 **고정 길이의 연속 벡터**로 변환하여 의미적 관계를 수치로 표현.

| 방식             | 설명                               |
| ---------------- | ---------------------------------- |
| **원-핫 인코딩** | 희소 벡터(대부분 0, 1개만 1)       |
| **임베딩**       | 학습된 밀집 벡터(의미적 거리 반영) |

> 예: “king – man + woman ≈ queen”

---

### 🔹 `nn.Embedding` 매개변수

| 파라미터        | 의미                        |
| --------------- | --------------------------- |
| `vocab_size`    | 단어 사전 크기 (총 토큰 수) |
| `embedding_dim` | 각 단어 벡터의 차원 수      |

> 예: `nn.Embedding(10000, 300)` → 1만 개 단어, 300차원 벡터

---

### 🔹 면접 포인트

* “임베딩 벡터는 왜 필요한가요?”
  → 단어 간 유사도를 학습시키기 위해.
* “원-핫 인코딩과 임베딩의 차이?”
  → 전자는 희소, 후자는 밀집 + 의미 내포.

---

## 🔁 4️⃣ **RNN / LSTM**

### 🔹 RNN (Recurrent Neural Network)

* 순차적 데이터(문장, 시계열)를 처리하기 위한 구조
* 이전 시점의 hidden state를 다음 입력과 함께 전달

📎 한계: **Vanishing Gradient Problem** (장기 의존성 소멸)

---

### 🔹 LSTM (Long Short-Term Memory)

* RNN의 단점을 해결한 구조
* **Cell State**(장기 기억)와 **게이트 구조**(입력/출력/망각) 사용

| 게이트      | 역할                         |
| ----------- | ---------------------------- |
| Forget Gate | 과거 정보 중 버릴 것 선택    |
| Input Gate  | 새 정보 중 저장할 것 선택    |
| Output Gate | 다음 단계로 전달할 정보 결정 |

> ✅ 시험 포인트:
> “LSTM은 RNN의 장기 의존성 문제를 해결하기 위해 게이트 메커니즘을 사용한다.”

---

### 🔹 입력/출력 형태

| 모델      | 입력 형태             | 출력 형태           |
| --------- | --------------------- | ------------------- |
| `nn.RNN`  | (batch, seq, feature) | hidden state        |
| `nn.LSTM` | (batch, seq, feature) | (hidden, cell) 튜플 |

> 🔹 LSTM은 hidden state 외에 **cell state** 추가로 더 안정적 학습 가능

---

## 🎯 5️⃣ **Attention Mechanism**

### 🔹 핵심 개념

RNN/LSTM은 전체 문맥을 요약(hidden state)해야 하지만,
Attention은 **입력 전체에서 중요한 부분에 “가중치”를 부여**하여 정보를 직접 참조.

---

### 🔹 어텐션의 작동 원리

![alt text](/assets/img/attention.png)


| 구성요소     | 의미                    |
| ------------ | ----------------------- |
| **Q(Query)** | 현재 시점 정보          |
| **K(Key)**   | 전체 입력의 인덱스 역할 |
| **V(Value)** | 실제 정보(값)           |
| **Softmax**  | 중요도(가중치) 계산     |

> ✅ 시험 포인트:
> “어텐션은 시퀀스 내 특정 단어에 집중하도록 가중치를 학습한다.”

---

### 🔹 주요 어텐션 종류

| 종류                       | 제안자 | 특징                              |
| -------------------------- | ------ | --------------------------------- |
| **Bahdanau (Additive)**    | 2015   | 추가 네트워크로 유사도 계산       |
| **Luong (Multiplicative)** | 2015   | 내적(dot product) 기반, 계산 빠름 |

---

## 🤗 6️⃣ **HuggingFace Transformers**

### 🔹 핵심 개념

사전 학습(Pretrained)된 NLP 모델을 쉽게 불러와 활용할 수 있는 라이브러리.
→ 토크나이징, 임베딩, 학습, 추론까지 **엔드투엔드 지원**

| 구성요소      | 역할                              |
| ------------- | --------------------------------- |
| **Tokenizer** | 텍스트 → 숫자 변환                |
| **Model**     | Transformer 기반 예측 수행        |
| **Trainer**   | 학습/평가 루프 자동화             |
| **Pipeline**  | 분류, 번역, 요약 등 태스크 자동화 |

---

### 🔹 주요 함수

| 함수                      | 역할                            |
| ------------------------- | ------------------------------- |
| `from_pretrained()`       | 사전 학습 모델 로드             |
| `pipeline("task")`        | 즉시 태스크 수행(예: 감정 분석) |
| `model.save_pretrained()` | fine-tuned 모델 저장            |

> 예:
>
> ```python
> from transformers import pipeline
> pipe = pipeline("sentiment-analysis")
> pipe("This movie was great!")
> ```

---

## 🧩 7️⃣ **Encoder / Decoder / Seq2Seq**

| 구조        | 대표 모델 | 설명                                      |
| ----------- | --------- | ----------------------------------------- |
| **Encoder** | BERT      | 입력 인코딩(문맥 요약)                    |
| **Decoder** | GPT       | 입력을 기반으로 출력 생성                 |
| **Seq2Seq** | T5, BART  | Encoder-Decoder 결합 구조 (번역, 요약 등) |

📘 **Attention 위치**

* Encoder 내부: Self-Attention
* Decoder 내부: Masked Self-Attention
* Encoder → Decoder: Cross-Attention

> ✅ 시험 포인트:
> “Transformer는 RNN을 완전히 제거하고, 어텐션만으로 문맥을 처리한다.”

---

## 📊 **시험 대비 핵심 요약표**

| 구분                | 핵심 키워드               | 주요 포인트                   |
| ------------------- | ------------------------- | ----------------------------- |
| **Tokenizer**       | BPE, WordPiece            | 문장 → 토큰, encode/decode    |
| **Embedding**       | Word2Vec, nn.Embedding    | 단어 의미를 벡터로 표현       |
| **RNN/LSTM**        | 순환, Cell State          | 시퀀스 처리, 장기 의존성 해결 |
| **Attention**       | Q·K·V, Softmax            | 문맥 가중치 계산              |
| **HuggingFace**     | from_pretrained, pipeline | 사전학습 모델 활용            |
| **Encoder/Decoder** | BERT/GPT/T5               | 입력 인코딩, 출력 생성        |

---

> 💡 **한줄 요약:**
> “토크나이저로 문장을 토큰화하고, 임베딩을 통해 의미 공간으로 매핑한다.
> RNN/LSTM은 순차 데이터를 처리하며, Attention은 정보 중요도를 학습한다.
> 최종적으로 HuggingFace는 이러한 구조를 쉽게 구현할 수 있는 통합 플랫폼이다.”


