
# ğŸ”¥ PyTorchë¥¼ í™œìš©í•œ MLP êµ¬í˜„í•˜ê¸°

## ğŸ“˜ MLP (Multi-Layer Perceptron) ê¸°ë³¸ ê°œë…

* **MLP**ëŠ” ê°€ì¥ ê¸°ë³¸ì ì¸ ì¸ê³µì‹ ê²½ë§ êµ¬ì¡°ë¡œ,
  ì…ë ¥ì¸µ(Input) â†’ ì€ë‹‰ì¸µ(Hidden) â†’ ì¶œë ¥ì¸µ(Output)ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.
* ê° ì¸µì—ì„œëŠ” `Linear(ê°€ì¤‘ì¹˜ ì—°ì‚°)` â†’ `ReLU(ë¹„ì„ í˜•ì„±)` â†’ `Dropout(ì¼ë¶€ ë‰´ëŸ° ë¹„í™œì„±í™”)` ìˆœìœ¼ë¡œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.

### âœ… êµ¬í˜„ êµ¬ì¡°

```python
class MLP(nn.Module):
    def __init__(self, input_dim, num_classes, hidden_dims=(128, 64), dropout=0.2):
        super().__init__()
        h1, h2 = hidden_dims
        self.net = nn.Sequential(
            nn.Linear(input_dim, h1),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(h1, h2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(h2, num_classes)
        )
    def forward(self, x):
        return self.net(x)
```

---

## ğŸ§  ê°ì²´ì— ()ë¥¼ ê±¸ë©´ ì–´ë–»ê²Œ ë ê¹Œ?

* `model = MLP(...)`
* `output = model(x)`
  â†’ ì‚¬ì‹¤ì€ ë‚´ë¶€ì ìœ¼ë¡œ **`model.__call__()`** â†’ **`model.forward()`**ê°€ í˜¸ì¶œë˜ëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤.
* ì¦‰, `()` ì—°ì‚°ìëŠ” ëª¨ë¸ì˜ **ìˆœì „íŒŒ(forward pass)** ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

---

## ğŸ§© ì‹¤ìŠµ: ìˆ«ì íŒë…ê¸° ë§Œë“¤ê¸° (MLP í•™ìŠµ ë£¨í”„)

### 1ï¸âƒ£ í•™ìŠµìš© í•¨ìˆ˜

```python
def train_one_epoch(model, loader, optimizer, device):
    model.train()                     # í•™ìŠµ ëª¨ë“œ
    running_loss, correct, total = 0, 0, 0

    for xb, yb in loader:
        xb, yb = xb.to(device), yb.to(device)
        y_targets = yb.squeeze(1).long()
        logits = model(xb)
        optimizer.zero_grad()
        loss = F.cross_entropy(logits, y_targets)
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * xb.size(0)
        preds = logits.argmax(dim=1)
        correct += (preds == y_targets).sum().item()
        total += xb.size(0)

    return running_loss / total, correct / total
```

### 2ï¸âƒ£ í‰ê°€ìš© í•¨ìˆ˜

```python
def evaluate(model, loader, device):
    model.eval()                      # ì¶”ë¡  ëª¨ë“œ
    running_loss, correct, total = 0, 0, 0
    all_preds, all_targets = [], []

    with torch.no_grad():             # ë¯¸ë¶„ ë¹„í™œì„±í™”
        for xb, yb in loader:
            xb, yb = xb.to(device), yb.to(device)
            y_targets = yb.squeeze(1).long()
            logits = model(xb)
            loss = F.cross_entropy(logits, y_targets)

            running_loss += loss.item() * xb.size(0)
            preds = logits.argmax(dim=1)
            correct += (preds == y_targets).sum().item()
            total += xb.size(0)
            all_preds.append(preds.cpu())
            all_targets.append(y_targets.cpu())

    return running_loss / total, correct / total
```

---

## âš™ï¸ model.train() vs model.eval()

| ëª¨ë“œ                  | Dropout/BatchNorm ë™ì‘       | ëª©ì                  |
| ------------------- | -------------------------- | ------------------ |
| **`model.train()`** | Dropout í™œì„±í™” (ë¬´ì‘ìœ„ë¡œ ë‰´ëŸ° ì¼ë¶€ ë”) | í•™ìŠµ ì¤‘ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ     |
| **`model.eval()`**  | Dropout/BN ê³ ì •, í‰ê· Â·ë¶„ì‚° ì‚¬ìš©    | ì˜ˆì¸¡(ì¶”ë¡ ) ì‹œ ì¼ê´€ëœ ê²°ê³¼ ë³´ì¥ |

> ğŸ’¡ ì¦‰, `train()`ì€ â€œí•™ìŠµìš©â€, `eval()`ì€ â€œí…ŒìŠ¤íŠ¸ìš© ëª¨ë“œâ€ ì „í™˜ ìŠ¤ìœ„ì¹˜ì…ë‹ˆë‹¤.

---

## â³ í•™ìŠµì´ ê¸¸ì–´ì§€ë©´â€¦?

* **ê³¼ì í•©(Overfitting)** ë°œìƒ ìœ„í—˜ì´ ìˆìŠµë‹ˆë‹¤.
  â†’ Validation lossê°€ ì¦ê°€í•˜ë©´ ì¡°ê¸° ì¢…ë£Œ(Early Stopping) ê³ ë ¤
* **Dropout**, **Learning Rate Scheduler**, **ì •ê·œí™”** ë“±ì„ í†µí•´ ì™„í™”í•©ë‹ˆë‹¤.
* GPU ì‚¬ìš© ì‹œ **í•™ìŠµ ì‹œê°„ ë‹¨ì¶• + batch í¬ê¸° ìµœì í™”**ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤.

---

## ğŸ” sklearn.classificationì— ìˆëŠ” ëª¨ë“ˆë¡œë„ ê³„ì‚°ì´ ë ê¹Œìš”?

ë„¤, ê°€ëŠ¥í•©ë‹ˆë‹¤.
PyTorchë¡œ í•™ìŠµí•œ ê²°ê³¼(`preds`, `targets`)ë¥¼ ë„˜íŒŒì´ ë°°ì—´ë¡œ ë³€í™˜í•´
`sklearn.metrics`ì˜ í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ë©´ ë©ë‹ˆë‹¤.

```python
from sklearn.metrics import classification_report, confusion_matrix

print(confusion_matrix(targets_cat, preds_cat))
print(classification_report(targets_cat, preds_cat))
```

---

## ğŸ’¬ ì „ì²´ íë¦„ ìš”ì•½

| ë‹¨ê³„                      | ë‚´ìš©                                            |
| ----------------------- | --------------------------------------------- |
| â‘  ë°ì´í„° ì „ì²˜ë¦¬               | `StandardScaler`ë¡œ ì •ê·œí™”, Tensor ë³€í™˜              |
| â‘¡ Dataset/DataLoader êµ¬ì„± | `TensorDataset` â†’ `DataLoader(batch_size)`    |
| â‘¢ ëª¨ë¸ ì •ì˜                 | `MLP(nn.Module)` êµ¬ì„± (Linear + ReLU + Dropout) |
| â‘£ í•™ìŠµ ë£¨í”„                 | `train_one_epoch()`ìœ¼ë¡œ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸               |
| â‘¤ ê²€ì¦ ë£¨í”„                 | `evaluate()`ë¡œ ì„±ëŠ¥ í™•ì¸                           |
| â‘¥ ì„±ëŠ¥ í‰ê°€                 | `sklearn.metrics`ë¡œ F1-score, ì •í™•ë„ í™•ì¸           |

---

## âœ… í•µì‹¬ í¬ì¸íŠ¸ ì •ë¦¬

* MLPëŠ” **ì…ë ¥-ì€ë‹‰-ì¶œë ¥ì¸µìœ¼ë¡œ êµ¬ì„±ëœ ê°€ì¥ ë‹¨ìˆœí•œ ì‹ ê²½ë§ êµ¬ì¡°**
* PyTorchì˜ í•µì‹¬ í•™ìŠµ êµ¬ì¡°ëŠ”
  **DataLoader â†’ Model â†’ Optimizer â†’ Loss â†’ Backpropagation**
* `model.train()` / `model.eval()` ë¡œ ëª¨ë“œë¥¼ ëª…ì‹œí•´ì•¼ Dropout/BNì´ ì •ìƒ ë™ì‘
* í•™ìŠµ í›„ ê²°ê³¼ë¥¼ `sklearn` ëª¨ë“ˆë¡œ ì†ì‰½ê²Œ í‰ê°€ ê°€ëŠ¥


