
# ğŸ§­ ë°ì´í„° ì •ê·œí™” ë° ì„ í˜•ëŒ€ìˆ˜ ê¸°ë°˜ í•´ë²• ì •ë¦¬

---

## ğŸ§® 1. ë°ì´í„° ì •ê·œí™” ë° ì¤€ë¹„

### ğŸ”¹ 1.1 ì—°ì†í˜• / ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„ë¥˜í•˜ê¸°

* **ì—°ì†í˜• ë³€ìˆ˜(Continuous)**: ìˆ˜ì¹˜í˜• ê°’ (ì˜ˆ: `age`, `price`, `income`)
* **ë²”ì£¼í˜• ë³€ìˆ˜(Categorical)**: ë²”ì£¼(label) ê°’ (ì˜ˆ: `gender`, `region`, `day`)

> `categorical_cols` ë¦¬ìŠ¤íŠ¸ë¥¼ ë”°ë¡œ ì •ì˜í•˜ì—¬ êµ¬ë¶„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
categorical_cols = df.select_dtypes('object').columns
```

---

### ğŸ§© 1.2 í‘œì¤€í™”(Standardization)ì˜ ì •ì˜

* ê° íŠ¹ì„±(feature)ì„ **í‰ê·  0, í‘œì¤€í¸ì°¨ 1**ë¡œ ìŠ¤ì¼€ì¼ë§í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.
* ìˆ˜ì‹:

![alt text](/assets/img/Standardization.png)

* ì´ìœ : ëª¨ë“  íŠ¹ì„±ì´ ê°™ì€ ë‹¨ìœ„ë¡œ ë¹„êµë˜ë„ë¡ í•˜ì—¬, íšŒê·€ë‚˜ ê±°ë¦¬ ê¸°ë°˜ ëª¨ë¸ì´ íŠ¹ì • ë³€ìˆ˜ì— ì¹˜ìš°ì¹˜ì§€ ì•Šê²Œ ë§Œë“­ë‹ˆë‹¤.

---

### ğŸ‘¨â€ğŸ’» 1.3 ì‹¤ìŠµ: í‘œì¤€í™” ì§„í–‰í•´ë³´ê¸°

```python
mean = X.mean(axis=0)
sigma = X.std(axis=0)

sigma_safe = np.where(sigma == 0, epsilon, sigma)  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
X_norm = (X - mean) / sigma_safe
```

> âš™ï¸ `sigma_safe`ëŠ” ë¶„ëª¨ê°€ 0ì´ ë˜ì§€ ì•Šë„ë¡ **Îµ(ì•„ì£¼ ì‘ì€ ìˆ˜)** ë¡œ ëŒ€ì²´í•œ ì•ˆì „í•œ í‘œì¤€í¸ì°¨ ë²¡í„°ì…ë‹ˆë‹¤.

---

### ğŸ§  1.4 `to_numpy()` ë©”ì„œë“œë€?

* pandas DataFrameì„ numpy ë°°ì—´ë¡œ ë³€í™˜í•˜ëŠ” ë©”ì„œë“œì…ë‹ˆë‹¤.
  â†’ íšŒê·€ ëª¨ë¸ì—ì„œ **í–‰ë ¬ ì—°ì‚°**ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ í•„ìˆ˜!

```python
X_array = df[['col1', 'col2']].to_numpy()
```

---

## ğŸ“˜ 2. ì„ í˜•ëŒ€ìˆ˜ë¥¼ ì´ìš©í•œ í•´ë²• (Linear Regression)

---

### ğŸ”¹ 2.1 ì •ê·œë°©ì •ì‹ (Normal Equation)

#### âœ… ê°œë…

* ì„ í˜•íšŒê·€ í•´ë¥¼ **ë¯¸ë¶„ ì—†ì´ í•œ ë²ˆì— êµ¬í•˜ëŠ” ë°©ë²•**ì…ë‹ˆë‹¤.
* ìµœì†Œì œê³±ì˜¤ì°¨(MSE)ë¥¼ ìµœì†Œí™”í•˜ëŠ” í•´:

![alt text](/assets/img/Normal_Equation.png)


#### âš ï¸ ì—­í–‰ë ¬ì´ í•­ìƒ ì¡´ì¬í•˜ì§€ëŠ” ì•ŠìŒ

* ( X )ê°€ ì •ì‚¬ê° í–‰ë ¬ì´ ì•„ë‹ ìˆ˜ë„ ìˆê³ ,
* ( X^T X )ê°€ **ì—­í–‰ë ¬ ë¶ˆê°€ëŠ¥(singular)** í•œ ê²½ìš°ë„ ìˆìŒ â†’ í•´ê²°ì±…ìœ¼ë¡œ **SVD** ì‚¬ìš©.

#### ğŸ§© ì‹¤ìŠµ

```python
XT_X = X_b.T @ X_b
XT_y = X_b.T @ y
theta = np.linalg.inv(XT_X) @ XT_y
```

---

### ğŸ’¡ ì ˆí¸í•­(bias)ê³¼ hstackì˜ ì˜ë¯¸

#### âœ… ì ˆí¸í•­

* íšŒê·€ì‹ ( y = a x + b ) ì—ì„œ **b(ì ˆí¸)** ì„ ì¶”ê°€í•˜ê¸° ìœ„í•´
  ( X ) í–‰ë ¬ì— **1ì˜ ì—´(column)** ì„ ì¶”ê°€í•©ë‹ˆë‹¤.

#### âœ… np.hstack

* ìˆ˜í‰ìœ¼ë¡œ ë°°ì—´ì„ ì´ì–´ ë¶™ì´ëŠ” í•¨ìˆ˜
  â†’ ì ˆí¸í•­ì„ ì¶”ê°€í•  ë•Œ ì‚¬ìš©.

```python
X_b = np.hstack([np.ones((m, 1)), X])
```

|   ì›ë˜ X   | np.ones((m,1)) |  â†’ hstack ê²°ê³¼  |
| :--------: | :------------: | :-------------: |
| xâ‚, xâ‚‚, xâ‚ƒ |       1        | [1, xâ‚, xâ‚‚, xâ‚ƒ] |

---

### ğŸ”¹ 2.2 ìµœì†Œì œê³±ë²• (Least Squares)

#### âœ… ìˆ˜í•™ì  ì •ì˜

* â€œëª¨ë“  ë°ì´í„°ì— ëŒ€í•´ ì˜¤ì°¨ ì œê³±ì˜ í•©ì´ ìµœì†Œê°€ ë˜ëŠ” Î¸ ì°¾ê¸°â€
* ìˆ˜ì¹˜ì ìœ¼ë¡œ ì•ˆì •ì ì¸ `np.linalg.lstsq()` í™œìš©:

```python
theta_lstsq, _, _, _ = np.linalg.lstsq(X_b, y, rcond=None)
```

#### ğŸ” MSE ê³„ì‚°

```python
y_pred = X_b @ theta_lstsq
mse = np.mean((y_pred - y) ** 2)
```

---

### ğŸ”¹ 2.3 SVDë¥¼ ì´ìš©í•œ ìµœì†Œì œê³± í•´ êµ¬í•˜ê¸°

#### âœ… ê°œë…

* **SVD(íŠ¹ì´ê°’ ë¶„í•´)** ëŠ” ì—­í–‰ë ¬ì´ ì¡´ì¬í•˜ì§€ ì•Šì•„ë„
  ( X^T X ) ì˜ ì•ˆì •ì ì¸ ì—­ì„ êµ¬í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.
* ìˆ˜ì‹:

![alt text](/assets/img/SVD.png)


#### ğŸ§© ì‹¤ìŠµ ì½”ë“œ

```python
U, S, Vt = np.linalg.svd(X_b, full_matrices=False)
S_plus = np.diag(1 / S)
theta_svd = Vt.T @ S_plus @ U.T @ y
```

---

## âš™ï¸ 3. ê²½ì‚¬í•˜ê°•ë²• (Gradient Descent)ê³¼ ì†ì‹¤ ê³„ì‚°

#### âœ… ê°œë…

* í•´ë¥¼ ì§ì ‘ ê³„ì‚°í•˜ì§€ ì•Šê³ , **ë°˜ë³µì ìœ¼ë¡œ ì†ì‹¤ì„ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ Î¸ë¥¼ ì—…ë°ì´íŠ¸**í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.
* ê¸°ë³¸ ì—…ë°ì´íŠ¸ ì‹:

![alt text](/assets/img/Gradient_Descent.png)


#### ğŸ’» ê¸°ë³¸ êµ¬í˜„ ì˜ˆì‹œ

```python
theta = np.zeros(n+1)
alpha = 0.01
iterations = 1000
loss_history = []

for i in range(iterations):
    y_pred = X_b @ theta
    error = y_pred - y.flatten()
    mse = np.mean(error ** 2)
    loss_history.append(mse)
    gradient = (2/m) * (X_b.T @ error)
    theta = theta - alpha * gradient
```

---

## ğŸ§® ì¶”ê°€ ì‹¬í™”: ë¯¸ë‹ˆë°°ì¹˜ + Gradient Accumulation

#### âœ… í•µì‹¬ ì•„ì´ë””ì–´

* ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì“°ì§€ ì•Šê³ , ì‘ì€ ë©ì–´ë¦¬(batch)ë¡œ ë‚˜ëˆ  í•™ìŠµ.
* ì—¬ëŸ¬ ë°°ì¹˜ë¥¼ ëª¨ì•„ì„œ í•œ ë²ˆì— ì—…ë°ì´íŠ¸ â†’ ë©”ëª¨ë¦¬ ì ˆì•½ + ì•ˆì •ì„± í–¥ìƒ.

#### ğŸ’» êµ¬í˜„ íë¦„

```python
for epoch in range(epochs):
    grad_accum = np.zeros_like(theta)
    accum_count = 0

    for start in range(0, m, batch_size):
        end = min(start + batch_size, m)
        X_batch = X_b[start:end]
        y_batch = y[start:end].ravel()

        y_pred = X_batch @ theta
        error  = y_pred - y_batch
        grad   = (2.0 / len(X_batch)) * (X_batch.T @ error)

        grad_accum += grad
        accum_count += 1

        if accum_count == accumulate_steps or end == m:
            theta -= alpha * (grad_accum / accum_count)
            grad_accum = np.zeros_like(theta)
            accum_count = 0
```

---

## ğŸ“Š MSE í‰ê°€ ë° ì‹œê°í™”

#### âœ… MSE (Mean Squared Error)

![alt text](/assets/img/MSE.png)


#### ğŸ’» ì‹œê°í™” í•¨ìˆ˜

```python
def plot_prediction(y_true, y_pred):
    sns.scatterplot(x=y_true, y=y_pred, alpha=0.5)
    sns.lineplot(x=[y.min(), y.max()],
                 y=[y.min(), y.max()],
                 linestyle="--", color="red")
```

---

## ğŸ“ˆ ì •ë¦¬ ìš”ì•½

| ë‹¨ê³„        | í•µì‹¬ ë‚´ìš©                  | ì£¼ìš” ë©”ì„œë“œ                       |
| ----------- | -------------------------- | --------------------------------- |
| ë°ì´í„° ì¤€ë¹„ | ì—°ì†í˜•/ë²”ì£¼í˜• êµ¬ë¶„, í‘œì¤€í™” | `.select_dtypes()`, `.to_numpy()` |
| ì •ê·œë°©ì •ì‹  | ì—­í–‰ë ¬ ê¸°ë°˜ í•´ êµ¬í•˜ê¸°      | `np.linalg.inv()`                 |
| ìµœì†Œì œê³±ë²•  | ì•ˆì •ì  ìˆ˜ì¹˜ í•´ë²•           | `np.linalg.lstsq()`               |
| SVD í•´ë²•    | ì—­í–‰ë ¬ ë¶ˆê°€ ì‹œ ëŒ€ì•ˆ        | `np.linalg.svd()`                 |
| ê²½ì‚¬í•˜ê°•ë²•  | ë°˜ë³µì  ì†ì‹¤ ìµœì†Œí™”         | `for loop`, `gradient update`     |
| ë¯¸ë‹ˆë°°ì¹˜ GD | íš¨ìœ¨ì  í•™ìŠµ                | `batch_size`, `accumulate_steps`  |

